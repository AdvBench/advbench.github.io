<!DOCTYPE html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script
    async
    src="https://www.googletagmanager.com/gtag/js?id=UA-178132094-1"
  ></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "UA-178132094-1");
  </script>

  <meta charset="UTF-8" />
  <!--  <meta name="viewport" content="width=device-width, initial-scale=1" />-->
  <meta name="viewport" content="width=1024" />
  <title>RobustBench: Adversarial robustness benchmark</title>
  <link
    rel="stylesheet"
    href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"
  />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <script
    src="https://kit.fontawesome.com/b939870cfb.js"
    crossorigin="anonymous"
  ></script>
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/foundation/6.4.3/css/foundation.min.css"
  />
  <link
    rel="stylesheet"
    href="https://cdn.datatables.net/1.10.21/css/dataTables.foundation.min.css"
  />

  <script
    type="text/javascript"
    src="https://code.jquery.com/jquery-3.5.1.js"
  ></script>
  <script
    type="text/javascript"
    src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.min.js
    "
  ></script>
  <script
    type="text/javascript"
    src="https://cdn.datatables.net/1.10.21/js/dataTables.foundation.min.js"
  ></script>
  <script>
    $(document).ready(function () {
      $("#leaderboard1").DataTable({
        lengthMenu: [15, 25, 50, 75, 100],
      });
    });
    $(document).ready(function () {
      $("#leaderboard2").DataTable({
        lengthMenu: [15, 25, 50, 75, 100],
      });
    });
  </script>

  <link rel="stylesheet" href="./css/main.css" />
</head>

<body>
  <div id="navbar" class="nav">
    <div class="fixed-width-navbar">
      <ul class="left">
        <li><a href="#">RobustBench</a></li>
      </ul>
      <ul class="right">
        <li><a href="#leaderboard">Leaderboard</a></li>
        <li><a href="#faq">FAQ</a></li>
        <li><a href="#contribute">Contribute</a></li>
        <li>
          <a href="https://github.com/RobustBench/robustbench">Model Zoo ðŸš€</a>
        </li>
      </ul>
    </div>
  </div>

  <!-- <hr class="toprule" /> -->

  <header>
    <div class="logo"><img src="./images/logo.png" alt="logo" /></div>
    <div class="title">RobustBench</div>
    <div class="description">
      A standardized benchmark for adversarial robustness
    </div>
  </header>
  <!-- <hr class="toprule" /> -->

  <!-- <hr class="title-rule" /> -->
  <div class="content">
    <section id="introduction">
      <div class="overview">
        <p class="doublealign">
          The goal of <strong>RobustBench</strong> is to systematically track
          the <em>real</em> progress in adversarial robustness. There are
          already
          <a
            href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html"
            >more than 2&#39;000 papers</a
          >
          on this topic, but it is still unclear which approaches really work
          and which only lead to
          <a href="https://arxiv.org/abs/1802.00420">overestimated robustness</a
          >. We start from benchmarking the \(\ell_\infty\)- and
          \(\ell_2\)-robustness since these are the most studied settings in the
          literature. We use
          <a href="https://github.com/fra31/auto-attack">AutoAttack</a>, an
          ensemble of white-box and black-box attacks, to standardize the
          evaluation (for details see <a href="https://arxiv.org/abs/2010.09670">our paper</a>). Additionally,
          we open source the
          <a href="https://github.com/RobustBench/robustbench"
            >RobustBench library</a
          >
          that contains models used for the leaderboard to facilitate their
          usage for downstream applications.
        </p>
        <div class="flexbox-container features">
          <div class="element">
            <div class="icon">
              <img
                src="https://img.icons8.com/wired/100/000000/leaderboard.png"
              />
            </div>
            <p>
              Up-to-date leaderboard based <br />
              on 30+ recent papers
            </p>
          </div>
          <div class="element">
            <div class="icon">
              <img
                src="https://img.icons8.com/ios-glyphs/80/000000/user-credentials.png"
              />
            </div>
            <p>
              Unified access to 20+ state-of-the-art <br />robust models via
              Model Zoo
            </p>
          </div>
        </div>
      </div>
      <div class="details">
        <div class="box usage">
          <p>Model Zoo</p>
          <div class="divider"><hr /></div>
          Check out the
          <a href="https://github.com/RobustBench/robustbench#model-zoo"
            >available models</a
          >
          and our
          <a href="https://github.com/RobustBench/robustbench#notebooks"
            >Colab tutorials</a
          >.
          <div class="codeblock">
            <div class="vspace10"></div>
            <!--            <pre><code></code>-->
            <!--!pip install git+https://github.com/RobustBench/robustbench-->

            <!--from robustbench.utils import load_model-->
            <!--model = load_model(model_name='Carmon2019Unlabeled')-->

            <!--from robustbench.data import load_cifar10-->
            <!--x_test, y_test = load_cifar10(n_examples=100)-->

            <!--!pip install -q git+https://github.com/fra31/auto-attack-->
            <!--from autoattack import AutoAttack-->
            <!--adversary = AutoAttack(model, norm='Linf', eps=8/255)-->
            <!--x_adv = adversary.run_standard_evaluation(x_test, y_test)-->
            <!-- HTML generated using hilite.me -->
            <div
              style="
                background: #ffffff;
                overflow: auto;
                width: auto;
                border: solid gray;
                border-width: 0em 0em 0em 0em;
                padding: 0.2em 0.6em;
              "
            >
              <pre
                style="margin: 0; line-height: 125%"
              ><span style="color: #888888">!pip install git+https://github.com/RobustBench/robustbench</span>

<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">robustbench.utils</span> <span style="color: #008800; font-weight: bold">import</span> load_model
model <span style="color: #333333">=</span> load_model(model_name<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;Carmon2019Unlabeled&#39;</span>)

<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">robustbench.data</span> <span style="color: #008800; font-weight: bold">import</span> load_cifar10
x_test, y_test <span style="color: #333333">=</span> load_cifar10(n_examples<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">100</span>)

<span style="color: #888888">!pip install git+https://github.com/fra31/auto-attack</span>
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">autoattack</span> <span style="color: #008800; font-weight: bold">import</span> AutoAttack
adversary <span style="color: #333333">=</span> AutoAttack(model, norm<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;Linf&#39;</span>, eps<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">8</span><span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">255</span>)
x_adv <span style="color: #333333">=</span> adversary<span style="color: #333333">.</span>run_standard_evaluation(x_test, y_test)
</pre>
            </div>
          </div>
        </div>
        <div class="box images">
          <p>Analysis</p>
          <div class="divider"><hr /></div>
          Check out <a href="https://arxiv.org/abs/2010.09670">our paper</a> with a detailed analysis.
          <div>
            <!--          <div class="scroller analysis-images">-->
            <img
              class="analysis"
              src="./images/aa_robustness_vs_venues.png"
              alt="robustness_vs_venues"
            />
            <!--            <img-->
            <!--              src="./images/aa_robustness_vs_standard.png"-->
            <!--              alt="robustness_vs_clean"-->
            <!--            />-->
          </div>
        </div>
      </div>
      <div class="vspace10"></div>
    </section>

    <section id="leaderboard">
      <div class="heading">
        <p>
          Leaderboard:
          <span class="heading-math">CIFAR-10, \( \ell_\infty = 8/255 \)</span>,
          Untargeted, AutoAttack
        </p>
      </div>

      <table id="leaderboard1" class="datatable" style="width: 100%">
        <thead>
          <tr>
            <th class="rank">Rank</th>
            <th class="method">Method</th>
            <th class="ca">
              Standard <br />
              accuracy
            </th>
            <th class="aa">
              Robust <br />
              accuracy
            </th>
            <th class="extra-data">Extra <br />data</th>
            <th class="arch">Architecture</th>
            <th class="venue">Venue</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="ranktd">1</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
              <br />
              <span class="td-footer">
                We show the robust accuracy reported in the paper since
                AutoAttack performs slightly worse (65.88%).
              </span>
            </td>
            <td class="catd">91.10%</td>
            <td class="aatd">65.87%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-70-16</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
          	<td class="ranktd">2</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
              <br />
              <span class="td-footer">
                We show the robust accuracy reported in the paper since
                AutoAttack performs slightly worse (62.80%).
              </span>
            </td>
            <td class="catd">89.48%</td>
            <td class="aatd">62.76%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
          	<td class="ranktd">3</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.01279" target="_blank"
                >Do Wider Neural Networks Really Help Adversarial Robustness?</a
              >
            </td>
            <td class="catd">87.67%</td>
            <td class="aatd">60.65%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-34-15</td>
            <td class="venuetd">arXiv, Oct 2020</td>
     	    </tr>
          <tr>
          	<td class="ranktd">4</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2004.05884" target="_blank"
                >Adversarial Weight Perturbation Helps Robust Generalization</a
              >
            </td>
            <td class="catd">88.25%</td>
            <td class="aatd">60.04%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
            <td class="ranktd">5</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.13736" target="_blank"
                >Unlabeled Data Improves Adversarial Robustness</a
              >
            </td>
            <td class="catd">89.69%</td>
            <td class="aatd">59.53%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">6</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
              <br />
              <span class="td-footer">
              	We show the robust accuracy reported in the paper since
                AutoAttack performs slightly worse (57.20%).
              </span>
            </td>
            <td class="catd">85.29%</td>
            <td class="aatd">57.14%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-70-16</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
            <td class="ranktd">7</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.10509" target="_blank"
                >HYDRA: Pruning Adversarially Robust Neural Networks</a
              >
            </td>
            <td class="catd">88.98%</td>
            <td class="aatd">57.14%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
          	<td class="ranktd">8</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
              <br />
              <span class="td-footer">
                We show the robust accuracy reported in the paper since
                AutoAttack performs slightly worse (56.86%).
              </span>
            </td>
            <td class="catd">85.64%</td>
            <td class="aatd">56.82%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-20</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
            <td class="ranktd">9</td>
            <td class="methoddt">
              <a
                href="https://openreview.net/forum?id=rklOg6EFwS"
                target="_blank"
                >Improving Adversarial Robustness Requires Revisiting
                Misclassified Examples</a
              >
            </td>
            <td class="catd">87.50%</td>
            <td class="aatd">56.29%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
          	<td class="ranktd">10</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2004.05884" target="_blank"
                >Adversarial Weight Perturbation Helps Robust Generalization</a
              >
            </td>
            <td class="catd">85.36%</td>
            <td class="aatd">56.17%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
            <td class="ranktd">11</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.13725" target="_blank"
                >Are Labels Required for Improving Adversarial Robustness?</a
              >
            </td>
            <td class="catd">86.46%</td>
            <td class="aatd">56.03%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">12</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1901.09960" target="_blank"
                >Using Pre-Training Can Improve Model Robustness and
                Uncertainty</a
              >
            </td>
            <td class="catd">87.11%</td>
            <td class="aatd">54.92%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">ICML 2019</td>
          </tr>
          <tr>
          	<td class="ranktd">13</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.00467" target="_blank"
                >Bag of Tricks for Adversarial Training</a
              >
            </td>
            <td class="catd">86.43%</td>
            <td class="aatd">54.39%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-20</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
            <td class="ranktd">14</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.08619" target="_blank"
                >Boosting Adversarial Training with Hypersphere Embedding</a
              >
            </td>
            <td class="catd">85.14%</td>
            <td class="aatd">53.74%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-20</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
            <td class="ranktd">15</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2011.11164" target="_blank"
                >Learnable Boundary Guided Adversarial Training</a
              >
              <br /><span class="td-footer"
                >Uses \(\ell_{\infty} \) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">88.70%</td>
            <td class="aatd">53.57%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-20</td>
            <td class="venuetd">arXiv, Nov 2020</td>
          </tr>
          <tr>
            <td class="ranktd">16</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.11242" target="_blank"
                >Attacks Which Do Not Kill Training Make Adversarial Learning
                Stronger</a
              >
            </td>
            <td class="catd">84.52%</td>
            <td class="aatd">53.51%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">ICML 2020</td>
          </tr>
          <tr>
            <td class="ranktd">17</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.11569" target="_blank"
                >Overfitting in adversarially robust deep learning</a
              >
            </td>
            <td class="catd">85.34%</td>
            <td class="aatd">53.42%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-20</td>
            <td class="venuetd">ICML 2020</td>
          </tr>
          <tr>
            <td class="ranktd">18</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.10319" target="_blank"
                >Self-Adaptive Training: beyond Empirical Risk Minimization</a
              >
              <br /><span class="td-footer"
                >Uses \(\ell_{\infty} \) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">83.48%</td>
            <td class="aatd">53.34%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
            <td class="ranktd">19</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1901.08573" target="_blank"
                >Theoretically Principled Trade-off between Robustness and
                Accuracy</a
              >
              <br /><span class="td-footer"
                >Uses \(\ell_{\infty}\) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">84.92%</td>
            <td class="aatd">53.08%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">ICML 2019</td>
          </tr>
          <tr>
            <td class="ranktd">20</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2011.11164" target="_blank"
                >Learnable Boundary Guided Adversarial Training</a
              >
              <br /><span class="td-footer"
                >Uses \(\ell_{\infty} \) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">88.22%</td>
            <td class="aatd">52.86%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">arXiv, Nov 2020</td>
          </tr>
          <tr>
            <td class="ranktd">21</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1907.02610v2" target="_blank"
                >Adversarial Robustness through Local Linearization</a
              >
              <br /><span class="td-footer"
                >We show the robust accuracy reported in the paper since
                AutoAttack performs slightly worse (52.84%).
              </span>
            </td>
            <td class="catd">86.28%</td>
            <td class="aatd">52.81%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-40-8</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">22</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2003.12862" target="_blank"
                >Adversarial Robustness: From Self-Supervised Pre-Training to
                Fine-Tuning</a
              >
              <br /><span class="td-footer">Uses ensembles of 3 models. </span>
            </td>
            <td class="catd">86.04%</td>
            <td class="aatd">51.56%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-50</td>
            <td class="venuetd">CVPR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">23</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.01278" target="_blank"
                >Efficient Robust Training via Backward Smoothing</a
              >
            </td>
            <td class="catd">85.32%</td>
            <td class="aatd">51.12%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
            <td class="ranktd">24</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2003.09347" target="_blank"
                >Improving Adversarial Robustness Through Progressive Hardening</a
              >
            </td>
            <td class="catd">86.84%</td>
            <td class="aatd">50.72%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">arXiv, Mar 2020</td>
          </tr>
          <tr>
            <td class="ranktd">25</td>
            <td class="methoddt">
              <a href="https://github.com/MadryLab/robustness" target="_blank"
                >Robustness library</a
              >
            </td>
            <td class="catd">87.03%</td>
            <td class="aatd">49.25%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-50</td>
            <td class="venuetd">GitHub,<br />Oct 2019</td>
          </tr>
          <tr>
            <td class="ranktd">26</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.05186" target="_blank"
                >Harnessing the Vulnerability of Latent Layers in Adversarially
                Trained Models</a
              >
            </td>
            <td class="catd">87.80%</td>
            <td class="aatd">49.12%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">IJCAI 2019</td>
          </tr>
          <tr>
            <td class="ranktd">27</td>
            <td class="methoddt">
              <a
                href="http://papers.nips.cc/paper/8339-metric-learning-for-adversarial-robustness"
                target="_blank"
                >Metric Learning for Adversarial Robustness</a
              >
            </td>
            <td class="catd">86.21%</td>
            <td class="aatd">47.41%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">28</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.00877" target="_blank"
                >You Only Propagate Once: Accelerating Adversarial Training via
                Maximal Principle</a
              >
            </td>
            <td class="catd">87.20%</td>
            <td class="aatd">44.83%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">29</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1706.06083" target="_blank"
                >Towards Deep Learning Models Resistant to Adversarial
                Attacks</a
              >
            </td>
            <td class="catd">87.14%</td>
            <td class="aatd">44.04%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">ICLR 2018</td>
          </tr>
          <tr>
            <td class="ranktd">30</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.10626" target="_blank"
                >Rethinking Softmax Cross-Entropy Loss for Adversarial
                Robustness</a
              >
            </td>
            <td class="catd">80.89%</td>
            <td class="aatd">43.48%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-32</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">31</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2001.03994" target="_blank"
                >Fast is better than free: Revisiting adversarial training</a
              >
            </td>
            <td class="catd">83.34%</td>
            <td class="aatd">43.21%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-18</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">32</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1904.12843" target="_blank"
                >Adversarial Training for Free!</a
              >
            </td>
            <td class="catd">86.11%</td>
            <td class="aatd">41.47%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">33</td>
            <td class="methoddt">
              <a
                href="https://openreview.net/forum?id=HkeryxBtPB"
                target="_blank"
                >MMA Training: Direct Input Space Margin Maximization through
                Adversarial Training</a
              >
            </td>
            <td class="catd">84.36%</td>
            <td class="aatd">41.44%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-4</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">34</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.11911" target="_blank"
                >Controlling Neural Level Sets</a
              >
              <br />
              <span class="td-footer"
                >Uses \(\ell_{\infty}\) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">81.30%</td>
            <td class="aatd">40.22%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-18</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">35</td>
            <td class="methoddt">
              <a
                href="http://openaccess.thecvf.com/content_CVPR_2019/html/Moosavi-Dezfooli_Robustness_via_Curvature_Regularization_and_Vice_Versa_CVPR_2019_paper"
                target="_blank"
                >Robustness via Curvature Regularization, and Vice Versa</a
              >
            </td>
            <td class="catd">83.11%</td>
            <td class="aatd">38.50%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-18</td>
            <td class="venuetd">CVPR 2019</td>
          </tr>
          <tr>
            <td class="ranktd">36</td>
            <td class="methoddt">
              <a
                href="http://papers.nips.cc/paper/8459-defense-against-adversarial-attacks-using-feature-scattering-based-adversarial-training"
                target="_blank"
                >Defense Against Adversarial Attacks Using Feature
                Scattering-based Adversarial Training</a
              >
            </td>
            <td class="catd">89.98%</td>
            <td class="aatd">36.64%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">NeurIPS 2019</td>
          </tr>
          <tr>
            <td class="ranktd">37</td>
            <td class="methoddt">
              <a
                href="https://openreview.net/forum?id=Syejj0NYvr&noteId=Syejj0NYvr"
                target="_blank"
                >Adversarial Interpolation Training: A Simple Approach for
                Improving Model Robustness</a
              >
            </td>
            <td class="catd">90.25%</td>
            <td class="aatd">36.45%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">OpenReview, Sep 2019</td>
          </tr>
          <tr>
            <td class="ranktd">38</td>
            <td class="methoddt">
              <a
                href="http://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html"
                target="_blank"
                >Adversarial Defense via Learning to Generate Diverse Attacks</a
              >
            </td>
            <td class="catd">78.91%</td>
            <td class="aatd">34.95%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-20</td>
            <td class="venuetd">ICCV 2019</td>
          </tr>
          <tr>
            <td class="ranktd">39</td>
            <td class="methoddt">
              <a
                href="https://openreview.net/forum?id=rJlf_RVKwr"
                target="_blank"
                >Sensible adversarial learning</a
              >
            </td>
            <td class="catd">91.51%</td>
            <td class="aatd">34.22%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">OpenReview, Sep 2019</td>
          </tr>
          <tr>
            <td class="ranktd">40</td>
            <td class="methoddt">
              <a
                href="http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html"
                target="_blank"
                >Bilateral Adversarial Training: Towards Fast Training of More
                Robust Models Against Adversarial Attacks</a
              >
            </td>
            <td class="catd">92.80%</td>
            <td class="aatd">29.35%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">ICCV 2019</td>
          </tr>
          <tr>
            <td class="ranktd">41</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1905.10510" target="_blank"
                >Enhancing Adversarial Defense by k-Winners-Take-All</a
              >
              <br /><span class="td-footer">
                Uses \(\ell_{\infty}\) = 0.031 â‰ˆ 7.9/255 instead of 8/255.
              </span>
            </td>
            <td class="catd">79.28%</td>
            <td class="aatd">18.50%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">DenseNet-121</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">42</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2003.04286" target="_blank"
                >Manifold Regularization for Adversarial Robustness</a
              >
            </td>
            <td class="catd">90.84%</td>
            <td class="aatd">1.35%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-18</td>
            <td class="venuetd">arXiv, Mar 2020</td>
          </tr>
          <tr>
            <td class="ranktd">43</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1904.00887" target="_blank"
                >Adversarial Defense by Restricting the Hidden Space of Deep
                Neural Networks</a
              >
            </td>
            <td class="catd">89.16%</td>
            <td class="aatd">0.28%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-110</td>
            <td class="venuetd">ICCV 2019</td>
          </tr>
          <tr>
            <td class="ranktd">44</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1912.10185" target="_blank"
                >Jacobian Adversarially Regularized Networks for Robustness</a
              >
            </td>
            <td class="catd">93.79%</td>
            <td class="aatd">0.26%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">45</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2006.07682" target="_blank"
                >ClusTR: Clustering Training for Robustness
              </a>
            </td>
            <td class="catd">91.03%</td>
            <td class="aatd">0.00%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">arXiv, Jun 2020</td>
          </tr>
          <tr>
            <td class="ranktd">46</td>
            <td class="methoddt">
              <a href="" target="_blank">Standardly trained model</a>
            </td>
            <td class="catd">94.78%</td>
            <td class="aatd">0.00%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">N/A</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section>
      <div class="heading">
        <p>
          Leaderboard:
          <span class="heading-math">CIFAR-10, \( \ell_2 = 0.5 \)</span>,
          Untargeted, AutoAttack
        </p>
      </div>
      <table id="leaderboard2" class="datatable" style="width: 100%">
        <thead>
          <tr>
            <th class="rank">Rank</th>
            <th class="method">Method</th>
            <th class="ca">
              Standard <br />
              accuracy
            </th>
            <th class="aa">
              Robust <br />
              accuracy
            </th>
            <th class="extra-data">Extra <br />data</th>
            <th class="arch">Architecture</th>
            <th class="venue">Venue</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="ranktd">1</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
            </td>
            <td class="catd">94.74%</td>
            <td class="aatd">80.53%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">WideResNet-70-16</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
            <td class="ranktd">2</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2010.03593" target="_blank"
                >Uncovering the Limits of Adversarial Training against
                Norm-Bounded Adversarial Examples</a
              >
            </td>
            <td class="catd">90.90%</td>
            <td class="aatd">74.50%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-70-16</td>
            <td class="venuetd">arXiv, Oct 2020</td>
          </tr>
          <tr>
          	<td class="ranktd">3</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2004.05884" target="_blank"
                >Adversarial Weight Perturbation Helps Robust Generalization</a
              >
            </td>
            <td class="catd">88.51%</td>
            <td class="aatd">73.66%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-34-10</td>
            <td class="venuetd">NeurIPS 2020</td>
          </tr>
          <tr>
            <td class="ranktd">4</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2003.09461" target="_blank"
                >Adversarial Robustness on In- and Out-Distribution Improves
                Explainability</a
              >
            </td>
            <td class="catd">91.08%</td>
            <td class="aatd">72.91%</td>
            <td class="datatd">&#9745;</td>
            <td class="archtd">ResNet-50</td>
            <td class="venuetd">ECCV 2020</td>
          </tr>
          <tr>
            <td class="ranktd">5</td>
            <td class="methoddt">
              <a href="https://github.com/MadryLab/robustness" target="_blank"
                >Robustness library</a
              >
            </td>
            <td class="catd">90.83%</td>
            <td class="aatd">69.24%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-50</td>
            <td class="venuetd">GitHub,<br />Sep 2019</td>
          </tr>
          <tr>
            <td class="ranktd">6</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/2002.11569" target="_blank"
                >Overfitting in adversarially robust deep learning</a
              >
            </td>
            <td class="catd">88.67%</td>
            <td class="aatd">67.68%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">ResNet-18</td>
            <td class="venuetd">ICML 2020</td>
          </tr>
          <tr>
            <td class="ranktd">7</td>
            <td class="methoddt">
              <a href="https://arxiv.org/abs/1811.09600" target="_blank"
                >Decoupling Direction and Norm for Efficient Gradient-Based L2
                Adversarial Attacks and Defenses</a
              >
            </td>
            <td class="catd">89.05%</td>
            <td class="aatd">66.44%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">CVPR 2019</td>
          </tr>
          <tr>
            <td class="ranktd">8</td>
            <td class="methoddt">
              <a
                href="https://openreview.net/forum?id=HkeryxBtPB"
                target="_blank"
                >MMA Training: Direct Input Space Margin Maximization through
                Adversarial Training</a
              >
            </td>
            <td class="catd">88.02%</td>
            <td class="aatd">66.09%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-4</td>
            <td class="venuetd">ICLR 2020</td>
          </tr>
          <tr>
            <td class="ranktd">9</td>
            <td class="methoddt">
              <a href="" target="_blank">Standardly trained model</a>
            </td>
            <td class="catd">94.78%</td>
            <td class="aatd">0.00%</td>
            <td class="datatd">&#215;</td>
            <td class="archtd">WideResNet-28-10</td>
            <td class="venuetd">N/A</td>
          </tr>
        </tbody>
      </table>
    </section>

    <div class="vspace30"></div>

    <section id="faq">
      <div class="heading">
        <p>FAQ</p>
      </div>
      
      <p class="qa-box">
        <span class="question"
          >&#10148; Wait, how does this leaderboard differ from the
          <a href="https://github.com/fra31/auto-attack">AutoAttack leaderboard</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          > The <a href="https://github.com/fra31/auto-attack">AutoAttack leaderboard</a> is maintained simultaneously 
          with the <strong>RobustBench</strong> L2 / Linf leaderboards by <a href="https://github.com/fra31">Francesco Croce</a>, and all 
          the changes to either of them will be synchronized (given that the 3 restrictions on the models are met 
          for the <strong>RobustBench</strong> leaderboard). One can see the current L2 / Linf <strong>RobustBench</strong> leaderboard as a 
          continuously updated fork of the <a href="https://github.com/fra31/auto-attack">AutoAttack leaderboard</a> extended 
          by adaptive evaluations, Model Zoo, and clear restrictions on the models we accept. And in the future, 
          we will extend <strong>RobustBench</strong> with other threat models and potentially with a different standardized attack 
          if it's shown to perform better than AutoAttack.
        </span>
      </p>
      
      <p class="qa-box">
        <span class="question"
          >&#10148; Wait, how is it different from
          <a href="https://www.robust-ml.org/">robust-ml.org</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          ><a href="https://www.robust-ml.org/">robust-ml.org</a> focuses on
          <em>adaptive</em> evaluations, but we provide a
          <strong>standardized benchmark</strong>. Adaptive evaluations are
          great (e.g., see
          <a href="https://arxiv.org/abs/2002.08347">Tramer et al., 2020</a>),
          but very time-consuming and cannot be standardized. Instead, we argue
          that one can estimate robustness accurately <em>without</em> adaptive
          attacks but for this one has to introduce some restrictions on the
          considered models. See <a href="https://arxiv.org/abs/2010.09670">our paper</a> for more details.
        </span>
      </p>

      <p class="qa-box">
        <span class="question"
          >&#10148; How is it related to libraries like
          <a href="https://github.com/bethgelab/foolbox">foolbox</a> /
          <a href="https://github.com/tensorflow/cleverhans">cleverhans</a> /
          <a href="https://github.com/BorealisAI/advertorch">advertorch</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          >These libraries provide implementations of different
          <em>attacks</em>. Besides the standardized benchmark,
          <strong>RobustBench</strong> additionally provides a repository of the
          most robust models. So you can start using the robust models in one
          line of code (see the tutorial
          <a
            href="https://github.com/RobustBench/robustbench#model-zoo-quick-tour"
            >here</a
          >).</span
        >
      </p>

      <p class="qa-box">
        <span class="question"
          >&#10148; Why is Lp-robustness still interesting in 2020? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          >There are numerous interesting applications of Lp-robustness that
          span transfer learning (<a href="https://arxiv.org/abs/2007.08489"
            >Salman et al. (2020)</a
          >,
          <a href="https://arxiv.org/abs/2007.05869">Utrera et al. (2020)</a>),
          interpretability (<a href="https://arxiv.org/abs/1805.12152"
            >Tsipras et al. (2018)</a
          >, <a href="https://arxiv.org/abs/1910.08640">Kaur et al. (2019)</a>,
          <a href="https://arxiv.org/abs/1906.00945">Engstrom et al. (2019)</a
          >), security (<a href="https://arxiv.org/abs/1811.03194"
            >TramÃ¨r et al. (2018)</a
          >,
          <a href="https://arxiv.org/abs/1906.07153"
            >Saadatpanah et al. (2019)</a
          >), generalization (<a href="https://arxiv.org/abs/1911.09665"
            >Xie et al. (2019)</a
          >, <a href="https://arxiv.org/abs/1909.11764">Zhu et al. (2019)</a>,
          <a href="https://arxiv.org/abs/2004.10934"
            >Bochkovskiy et al. (2020)</a
          >), robustness to unseen perturbations (<a
            href="https://arxiv.org/abs/1911.09665"
            >Xie et al. (2019)</a
          >, <a href="https://arxiv.org/abs/1905.01034">Kang et al. (2019)</a>),
          stabilization of GAN training (<a
            href="https://arxiv.org/abs/2008.03364"
            >Zhong et al. (2020)</a
          >).</span
        >
      </p>

      <p class="qa-box">
        <span class="question"
          >&#10148; Does this benchmark only focus on Lp-robustness? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          >Lp-robustness is the most well-studied area, so we focus on it first.
          However, in the future, we plan to extend the benchmark to other
          perturbations sets beyond Lp-balls.</span
        >
      </p>

      <p class="qa-box">
        <span class="question"
          >&#10148; What about verified adversarial robustness? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          >We specifically focus on defenses which improve
          <em>empirical</em> robustness, given the lack of clarity regarding
          which approaches really improve robustness and which only make some
          particular attacks unsuccessful. For methods targeting verified
          robustness, we encourage the readers to check out
          <a href="https://arxiv.org/abs/1902.08722">Salman et al. (2019)</a>
          and <a href="https://arxiv.org/abs/2009.04131">Li et al. (2020)</a>.
        </span>
      </p>

      <p class="qa-box">
        <span class="question"
          >&#10148; What if I have a better attack than the one used in this
          benchmark? ðŸ¤”
        </span>
        <br />
        <span class="answer"
          >We will be happy to add a better attack or any adaptive evaluation
          that would complement our default standardized attacks.</span
        >
      </p>
    </section>



    <div class="vspace50"></div>

    <section id="citation">
      <div class="heading">
        <p>Citation</p>
      </div>
      Consider citing our whitepaper if you want to reference our leaderboard or if you are using the models from the Model Zoo:
<!--      @article{croce2020robustbench,-->
<!--        title={RobustBench: a standardized adversarial robustness benchmark},-->
<!--        author={Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Matthias Hein},-->
<!--        journal={arXiv preprint arXiv:2010.09670},-->
<!--        year={2020}-->
<!--      }-->
      <!-- HTML generated using hilite.me -->
      <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.0em .0em .0em .0em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #555555; font-weight: bold">@article</span>{croce2020robustbench,
    title<span style="color: #333333">=</span>{RobustBench: a standardized adversarial robustness benchmark},
    author<span style="color: #333333">=</span>{Croce, Francesco <span style="color: #000000; font-weight: bold">and</span> Andriushchenko, Maksym <span style="color: #000000; font-weight: bold">and</span> Sehwag, Vikash <span style="color: #000000; font-weight: bold">and</span> Flammarion, Nicolas
    <span style="color: #000000; font-weight: bold">and</span> Chiang, Mung <span style="color: #000000; font-weight: bold">and</span> Mittal, Prateek <span style="color: #000000; font-weight: bold">and</span> Matthias Hein},
    journal<span style="color: #333333">=</span>{arXiv preprint arXiv:2010.09670},
    year<span style="color: #333333">=</span>{2020}
}</pre></div>

    </section>

    <div class="vspace50"></div>

    <section id="contribute">
      <div class="details">
        <div class="box2">
          <p>Contribute to RobustBench!</p>
          <div class="divider"><hr /></div>
          We welcome any contribution in terms of both new robust models and
          evaluations. Please check
          <a href="https://github.com/RobustBench/robustbench#how-to-contribute"
            >here</a
          >
          for more details.
          <br />
          <br />
          Feel free to contact us at
          <a href="mailto:adversarial.benchmark@gmail.com"
            >adversarial.benchmark@gmail.com</a
          >
        </div>
        <div class="box2">
          <p>Maintainers</p>
          <div class="divider"><hr /></div>
          <ul>
            <li>
              <a href="https://twitter.com/fra__31" target="_blank"
                >Francesco Croce
              </a>
              <a href="https://twitter.com/fra__31"
                ><i class="fas fa-globe"></i
              ></a>
              <a href="https://github.com/fra31"
                ><i class="fab fa-github"></i
              ></a>
              <a href="https://scholar.google.com/citations?user=laq9cq0AAAAJ"
                ><i class="ai ai-google-scholar"></i
              ></a>
            </li>
            <li>
              <a
                href="https://people.epfl.ch/maksym.andriushchenko"
                target="_blank"
                >Maksym Andriushchenko</a
              >
              <a href="https://people.epfl.ch/maksym.andriushchenko"
                ><i class="fas fa-globe"></i
              ></a>
              <a href="https://github.com/max-andr"
                ><i class="fab fa-github"></i
              ></a>
              <a href="https://scholar.google.com/citations?user=ZNtuJYoAAAAJ"
                ><i class="ai ai-google-scholar"></i
              ></a>
            </li>
            <li>
              <a href="https://vsehwag.github.io/" target="_blank"
                >Vikash Sehwag</a
              >
              <a href="https://vsehwag.github.io/"
                ><i class="fas fa-globe"></i
              ></a>
              <a href="https://github.com/VSehwag"
                ><i class="fab fa-github"></i
              ></a>
              <a href="https://scholar.google.com/citations?user=JAkeEG8AAAAJ"
                ><i class="ai ai-google-scholar"></i
              ></a>
            </li>
          </ul>
        </div>
      </div>
    </section>
  </div>

  <hr class="bottomrule" />

  <footer>
    <small
      >&copy; 2020, RobustBench;
      <a href="https://icons8.com/icon/100413/access"
        >Icons from Icons8</a
      ></small
    >
  </footer>

  <script>
    // When the user scrolls the page, execute myFunction
    window.onscroll = function () {
      myFunction();
    };
    // Get the navbar
    var navbar = document.getElementById("navbar");
    // Get the offset position of the navbar
    var sticky = navbar.offsetTop;
    // Add the sticky class to the navbar when you reach its scroll position. Remove "sticky" when you leave the scroll position
    function myFunction() {
      if (window.pageYOffset >= sticky) {
        navbar.classList.add("sticky");
      } else {
        navbar.classList.remove("sticky");
      }
    }
  </script>
</body>
